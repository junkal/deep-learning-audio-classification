{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834a4833",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from data_processing import *\n",
    "from util import *\n",
    "\n",
    "from warnings import simplefilter\n",
    "# ignore all future warnings\n",
    "simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5644ed1",
   "metadata": {},
   "source": [
    "# Feature extraction from audio files\n",
    "\n",
    "The original data given by the GTZAN Dataset are in .wav format. In Kaggle, there is also the csv files that hold the extracted data from the audio files. The script here is to re-generate the csv file from the input .wav audio files. \n",
    "\n",
    "**There is a corrupted audio file (jazz.00054.wav) within the GTZAN Dataset that is downloaded from Kaggle**\n",
    "\n",
    "There are two parts to the feature extraction from the audio files\n",
    "\n",
    "## 1. Splitting up the audio files into 3-seconds blocks\n",
    "\n",
    "The function **extract_audio_chunks** from the data_processing.py splits up an audio file into smaller blocks. The main purpose to this is to create more more data samples for model training. Splitting up a 30 seconds audio file into 3-seconds blocks will increase the data sample by 10 times. The length is of each small audio chunk is defined by the seconds_per_chunk parameters of the extract_audio_chunks function. This function requires the pydub library. The splitting up of the audio file into small chunks is done explicity by calling **make_chunks** within the extract_audio_chunks function. The **make_chunks** function returns an array of the small audio chunks that are splitted up from the main audio file. \n",
    "\n",
    "These audio chunks are then exported as .wav file and stored in a temporary folder location for the feature extraction in step 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17429719",
   "metadata": {},
   "source": [
    "## 2. Extracting the features from each of the 3-seconds blocks\n",
    "\n",
    "The **extract_features** function from the data_processing.py extracts the features from each audio file. The function requires the **librosa** library.\n",
    "\n",
    "The features extracted are:\n",
    "* file name: extracted from the given file name\n",
    "* length: calculated from librosa.get_duration * librosa.get_samplerate \n",
    "* chroma_stft\n",
    "* rms\n",
    "* spectral_centroid\n",
    "* spectral_bandwidth\n",
    "* spectral_rolloff\n",
    "* zero_crossing_rate\n",
    "* harmonic\n",
    "* tempo\n",
    "* mfcc (20 instances)\n",
    "* label\n",
    "\n",
    "The means and variance are calculated for the following features:\n",
    "* chroma_stft\n",
    "* rms\n",
    "* spectral_centroid\n",
    "* spectral_bandwidth\n",
    "* spectral_rolloff\n",
    "* zero_crossing_rate\n",
    "* harmonic\n",
    "* tempo (only the mean is calculated for this feature)\n",
    "* mfcc (20 instances)\n",
    "\n",
    "Essentially, the extracted features are stored as Pandas dataframe. Each audio file feature should have the shape of (1, 58):\n",
    "* file name\n",
    "* length\n",
    "* chroma_stft_mean\n",
    "* chroma_stft_var\n",
    "* rms_mean\n",
    "* rms_var\n",
    "* spectral_centroid_mean\n",
    "* spectral_centroid_var\n",
    "* spectral_bandwidth_mean\n",
    "* spectral_bandwidth_var\n",
    "* rolloff_mean\n",
    "* rolloff_var\n",
    "* zero_crossing_rate_mean\n",
    "* zero_crossing_rate_var\n",
    "* harmonic_mean\n",
    "* harmonic_var\n",
    "* tempo\n",
    "* mfcc (20 instances, of mean and variance)\n",
    "* label\n",
    "\n",
    "Finally, the extracted features are saved as a .csv file to be used for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc9b439",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = './data/'\n",
    "data = pd.DataFrame()\n",
    "temp_dir = os.path.abspath(\"temp\")\n",
    "file_count = 0\n",
    "\n",
    "\n",
    "for dirpath, dirnames, filenames in os.walk(dataset_path):\n",
    "    for i, filename in enumerate(filenames):\n",
    "        if filename.endswith('.wav'):\n",
    "            filepath = os.path.join(dirpath, filename)\n",
    "            file_count += 1\n",
    "            print(\"Splitting {}\".format(filepath))\n",
    "            audio_chunks = extract_audio_chunks(filepath, 3000, temp_dir)\n",
    "            for audio_chunk in audio_chunks:\n",
    "                data = data.append(extract_features(audio_chunk), ignore_index=True)\n",
    "\n",
    "print(\"Total audio files processed: {}\".format(file_count))\n",
    "clear_folder(temp_dir)\n",
    "\n",
    "data.to_csv(\"audio_features_3_sec.csv\", header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cbc43a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
